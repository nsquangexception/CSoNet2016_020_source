\section{Experimental results} \label{experiments}

In this section, we present two experiments to evaluate the performance of our model on the aspect identification and sentiment classification tasks.

\subsection{Data}

To evaluate our model performance, we used Restaurant Review Dataset~\cite{data_ganu}.
This data is also used widely in previous work~\cite{data_ganu,Brody_Elhadad,Zhao}.
Data contain reviews about the restaurant with 1,644,923 tokens and 52,574 documents in total.
Documents in this dataset are annotated with one or more labels from a gold standard label set \textit{S = \{Food, Staff, Ambience, Price, Anecdote, Miscellaneous\}}.

\subsection{Aspect Extraction}
\subsubsection{Experimental Setup}

Following the previous studies (Brody and Elhadad~\cite{Brody_Elhadad} and Zhao et al.~\cite{Zhao}), reviews with less than 50 sentences are chosen.
From that, we only use sentences with a single label for evaluation to avoid ambiguity.
These sentences are selected from reviews with three major aspects chosen from the gold standard labels \textit{S' = \{Food, Staff, Ambience\}}.
After choosing suitable sentences from the data, we have 50303, 18437, 10018 sentences which labeled Food, Staff, Ambience.
Then, we change the case of any document to lower case and remove stop words.

To convert each word in the sentence into vector form, we use Word Embedding technique~\cite{rehurek_lrec_word2vec} combined with Google pre-trained model\footnote{\url{https://code.google.com/archive/p/word2vec/}}.
This model has been trained on part of Google News dataset (about 100 billion words).
It contains 300-dimensional vectors for 3 million words and phrases.
Each sentence's representation is a vector which is a sum of vectors that represent for words appeared in that sentence~\cite{Quoc_Le}.

We use 300 visible units in our WE-RBM as aspects identifying units, where units 1-100 capture \textit{Food} aspect, units 101-200 capture \textit{Staff} aspect and units 201-300 capture \textit{Ambience} aspect.
For sentiment classification, we also use 200 visible units, where units 1-100 capture positive information, units 101-200 represent negative information.
Initially, these units are set to 0.
After Gibbs sampling process, we create a sum for each group of 100 units to determine the aspect and sentiment polarity appeared in the document.


\subsubsection{Evaluation}

To evaluate the model's performance, we use Precision, Recall and $F_1$ scores for each aspect identification on Restaurant Review Dataset.

As a baseline, we implement \textit{Prior knowledge only} method, which only uses cosine similarity of the document vector and aspect vector to identify aspect.
For detail, we generate the vectors of ``food", ``staff" and ``ambience" words using WEM.
Then, we compute the cosine similarity between the vector of document $d_i$ and each of those 3 aspect vectors.
The document will be put into aspect $a_i$ category if the cosine similarity between $vec(d_i)$ and $vec(a_i)$ is the highest one.

We also use SVM, a well-known machine learning technique~\cite{libsvm}, to compare with our WE-RBM model.
We implement SVM by LIBSVM\footnote{\url{https://www.csie.ntu.edu.tw/~cjlin/libsvm/}} which is an integrated tool for support vector classification.
With linear kernel, SVM model is trained with feature vectors generated by bag-of-words model.
Without prior knowledge, standard RBM using Word Embeddings is also re-implemented.
This method processes the same Restaurant Review Dataset and identifies aspects for every document in this dataset under the same experimental conditions.
Evaluation results for aspect identification are given in Table~\ref{table_aspect_result}.

%\vspace*{-20pt}
\begin{table}
	\centering
	\caption{Aspect identification results in terms of precision, recall, and $F_1$ scores on the restaurant reviews dataset}
	\begin{tabular}{|C{1.5cm}|L{4.0cm}|R{1.8cm}|R{1.4cm}|R{1.2cm}|}
		\hline
		\multicolumn{1}{|c|}{\textbf{Aspect}} 
		&\multicolumn{1}{c|}{\textbf{Method}}                                            &\multicolumn{1}{c|}{\textbf{Precision}}
		&\multicolumn{1}{c|}{\textbf{Recall}}                              
		&\multicolumn{1}{c|}{$\mathbf{F_1}$}\\\hline
		\multirow{4}{*}{Food}&Prior knowledge only& 88.09 & 87.60 & 87.84\\
		&SVM & \textbf{94.10} & 75.74 & 83.93\\ 
		&RBM + Word Embeddings & 74.58 & 14.79 & 24.68\\
		&WE-RBM & 84.04 & \textbf{94.43} & \textbf{88.93}\\\hline
		\multirow{4}{*}{Staff}& Prior knowledge only & \textbf{95.96} & 45.39 & 61.63 \\ 
		&SVM & 90.25 & \textbf{78.14} & \textbf{83.76}\\ 
		&RBM + Word Embeddings & 27.73 & 53.32 & 36.48\\  
		&WE-RBM & 88.27 & 65.28 & 75.06\\\hline
		\multirow{4}{*}{Ambience}& Prior knowledge only & 16.02 & \textbf{86.93} & 27.06\\
		&SVM & 29.15 & 82.70 & 43.11\\ 
		&RBM + Word Embeddings & 14.85 & 60.12 & 23.81 \\  
		&WE-RBM & \textbf{69.22} & 59.64 & \textbf{64.07} \\\hline
	\end{tabular}
	\label{table_aspect_result}
\end{table}
%\vspace*{-20pt}

\subsubsection{Discussion}

Considering the results from Table~\ref{table_aspect_result}, we find that WE-RBM performs better than other methods.
Specifically, it is evident that our WE-RBM model outperforms previous methods' $F_1$ scores on \textit{Food} and \textit{Ambience} aspects.
Compared with Prior knowledge only, the $F_1$ scores improve by 1.09\%, 13.43\% and 37.01\% respectively, for the \textit{Food}, \textit{Staff}, and \textit{Ambience} aspects.
This result proves that our WE-RBM model is not entirely based on Prior knowledge obtained from WEM to have the better performance.
Inheriting RBM's ability in modeling latent topics and WEM's capability in identifying aspects, WE-RBM model can achieve higher Precision and Recall scores for the imbalanced dataset.
Compared with RBM using Word Embeddings, the $F_1$ scores yield relative improvements by 63.25\%, 38.58\%, and 40.26\% respectively, on the same aspects.
This result reveals that RBM model performs badly without the prior knowledge obtained from WEM.

Comparing with SVM performance, precision scores in food and staff domains of SVM are higher than WE-RBM's.
But SVM can not address the imbalanced data problem, which results in reduced precision in \textit{Ambience} domain.
Moreover, WE-RBM is a joint model which has ability to identify aspects and classify sentiment polarities simultaneously, while SVM is a classification model and we have to train two separated models to solve these two tasks.

In our model's performance evaluation process, we can not re-implement SERBM due to insufficient computational resources.
However, we compare WE-RBM's result with SERBM's result which was presented by Wang et al.~\cite{serbm}.
With the same settings in aspect identification task on Restaurant Review Dataset, WE-RBM outperforms SERBM with improvement in $F_1$ scores by 1.73\% and 7.06\% on \textit{Food} and \textit{Staff} aspects, respectively.

% ======================================================================================
\subsection{Sentiment Classification}

% Giới thiệu
Sentiment classification task is modified similar to aspect identification task in our WE-RBM model.
We assign a sentiment score to every document in the Restaurant Review Dataset based on the output of WE-RBM's sentiment identifying units in the visible layer.
Then we use SentiWordNet\footnote{\url{http://sentiwordnet.isti.cnr.it}}, a famous lexical resource for opinion mining~\cite{senti_wordnet}, and adapt SVM to compare the result with our WE-RBM model.

%Đối với mỗi thứ thì cài đặt và chạy như thế nào
Following the previous study~\cite{serbm}, we consult SentiWordNet to obtain a sentiment label for every word and aggregate these to judge the sentiment information of an entire review document in terms of the sum of word-specific scores.
For SVM, we use the linear kernel to train the model and the other setting is the same as WE-RBM's.
Table~\ref{sentiment_table_result} shows the comparison between SentiWordNet, SVM and WE-RBM with Accuracy as the evaluation metric.

%\vspace*{-10pt}
\begin{table}[h]
	\centering
	\caption{Accuracy of SentiWordNet, SVM and WE-RBM on sentiment classification task}
	\begin{tabular}{|C{3.0cm}|R{2.5cm}|}
		\hline
		\textbf{Method}   & \multicolumn{1}{c|}{\textbf{Accuracy}}
		\\
		\hline
		SentiWordNet & 73.36        
		\\ 
		\hline
		SVM & 78.26
		\\ 
		\hline
		WE-RBM & \textbf{79.79}
		\\
		\hline
	\end{tabular}
	\label{sentiment_table_result}
\end{table}
%\vspace*{-25pt}

As we can observe in Table~\ref{sentiment_table_result}, the best sentiment classification accuracy result is 79.79\% achieved by WE-RBM.
Compared with two baselines, our WE-RBM yields a relative improvement in the overall accuracy by 6.43\% over SentiWordNet and by 1.53\% over SVM.
Comparing the result of WE-RBM with SERBM's which was presented by Wang et al.~\cite{serbm}, WE-RBM increases the classification accuracy by 0.99\%. 

The reason for better performance of WE-RBM compared with other methods is the combination of prior knowledge and generating process of RBM.
Prior knowledge can be considered as the first classification phase of WE-RBM.
This knowledge boosts WE-RBM classifier in both aspect and sentiment identification.